[[install-planning]]
= 规划您的安装
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

通过运行一系列可运行的playbooks来安装{product-title}。在准备安装集群时,
创建一个清单文件,该文件表示环境和{product-title}集群配置。
虽然熟悉Ansible可能会使这个过程更容易,但这不是必需的。

您可以在
link:http://docs.ansible.com/ansible/[官方文档]中阅读更多关于Ansible及其基本用法的信息。

[[inital-planning]]
== 最初的计划

在安装production-product-title集群之前,需要回答以下问题:

ifdef::openshift-origin[]
* _您是在本地安装,还是在公共云中安装,还是在私有云中安装?_ The xref:planning-cloud-providers[安装方法]
小节提供了关于可用的云提供商选项的更多信息
endif::[]

* _在您的集群中有多少的pod?_ The xref:sizing[分级考虑]
小节为节点和pod提供了限制,这样您就可以计算您的环境需要多大。

* _在集群中需要多少主机?_ The xref:environment-scenarios[环境场景]
小节提供了单个主配置和多个主配置的多个示例。

* _是否需要一个xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#high-availability-masters[高可用]集群?_
高可用性配置提高容错能力。在这种情况下,可以使用:multi-masters-using-native-ha-colocated[多个Master使用本机HA]
示例来设置环境。

* _xref:../install_config/prometheus_cluster_monitoring.adoc#prometheus-cluster-monitoring[集群监测]是必须的吗?_
监控栈需要额外的
xref:../scaling_performance/scaling_cluster_monitoring.adoc#scaling-performance-cluster-monitoring[系统资源]。
注意监视堆栈是默认安装的。
获取更多信息,请参考
xref:../install_config/prometheus_cluster_monitoring.adoc#prometheus-cluster-monitoring[集群监测文档]。

* _您想使用Red Hat Enterprise Linux (RHEL)或RHEL Atomic Host为集群节点的操作系统吗?_
如果在RHEL上安装{product-title},则使用基于rpm的安装。
在RHEL原子主机上,使用系统容器。
xref:planning-installation-types[两种安装类型]提供了一个工作环境{product-title}。

* _您提供哪个身份
xref:../install_config/configuring_authentication.adoc#install-config-configuring-authentication[认证服务器]程序?_
如果您已经使用受支持的标识提供程序,请在安装期间配置{product-title}以使用该标识提供程序。

ifdef::openshift-enterprise[]
* _如果我将它与其他技术集成,是否支持我的安装?_
请查看
link:https://access.redhat.com/articles/2176281[OpenShift容器平台测试集成]
以获得测试集成的列表。
endif::[]

ifdef::openshift-origin[]
[[planning-cloud-providers]]
=== 与云提供商相比

您可以在本地安装{product-title},或者将其托管在共有云或私有云上,您可以使用所提供的
Ansible playbooks来帮助您自动化配置和安装过程。
更多信息,请参考
xref:running_install.adoc#advanced-cloud-providers[Running Installation Playbooks].
endif::[]

[[sizing]]
== 分级考虑

确定您的{product-title}集群需要多少节点和吊舱。
集群可伸缩性与集群环境中的pod数量相关。这个数字会影响设置中的其他数字。
请参考
xref:../scaling_performance/cluster_limits.adoc#scaling-performance-cluster-limits[集群限制]
针对{product-title}中对象的最新限制。

[[environment-scenarios]]
== 环境场景

使用这些环境场景来帮助根据您的规模需求计划您的{product-title}集群。

[NOTE]
====
不支持在安装之后从单个主集群移动到多个主集群。
====

在所有环境中,如果etcd主机与主主机位于同一位置,etcd将作为主机上的一个静态pod运行。
如果您的etcd主机没有与主主机共存,则它们将etcd作为独立进程运行。

[NOTE]
====
如果使用RHEL原子主机,则只能在主主机上配置etcd。
====

[[single-master-single-box]]
=== 一个系统上的单个主节点和节点

您可以在单个系统上安装{product-title},只用于开发环境,您不能将
_all-in-one environment_环境用作生产环境。

[[single-master-multi-node]]
=== 单个主节点和多个节点

下表描述了一个示例环境
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[主机] (在同一主机上安装etcd)
和两个
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[节点]:

[options="header"]
|===

|主机名 |要安装的基础设施组件

|*master.example.com*
|Master, etcd, and node

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

////
[[single-master-multi-etcd-multi-node]]
=== 单个主节点,多个etcd和多个节点

下表描述了一个示例环境
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[主机],
三个独立的
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[etcd]
主机,和两个
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[节点]:

[options="header"]
|===

|主机名称 |要安装的基础设施组件

|*master.example.com*
|Master and node

|*etcd1.example.com*
.3+.^|etcd

|*etcd2.example.com*

|*etcd3.example.com*

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

////

[[multi-masters-using-native-ha-colocated]]
=== 使用本机HA的多个主机

下面描述了3个示例环境
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[主机],
一个HAProxy负载均衡器,两个
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[节点]
使用 `原生` HA方法。etcd在主节点上作为静态pod运行:

[options="header"]
|===

|主机名称 |要安装的基础设施组件

|*master1.example.com*
.3+.^|主节点(使用本机HA集群)和节点以及集群etcd

|*master2.example.com*

|*master3.example.com*

|*lb.example.com*
|用于负载平衡API主端点的HAProxy

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

[[multi-masters-using-native-ha]]
=== 多个主机使用本机HA和外部集群etcd

下面描述了3个示例环境
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[masters],
一个HAProxy负载均衡器,三个外部集群
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[etcd]
主机,两个
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[nodes]
使用`原生`HA方法:

[options="header"]
|===

|主机名称 |要安装的基础设施组件

|*master1.example.com*
.3+.^|主节点(使用本机HA集群)和节点

|*master2.example.com*

|*master3.example.com*

|*lb.example.com*
|用于负载平衡API主端点的HAProxy

|*etcd1.example.com*
.3+.^|etcd集群

|*etcd2.example.com*

|*etcd3.example.com*

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

[[planning-stand-alone-registry]]
=== 独立的注册表

还可以安装{product-title},使用{product-title}的集成注册表作为独立注册表。
有关更多详细信息,请查看
xref:stand_alone_registry.adoc#install-config-installing-stand-alone-registry[安装独立注册表]。

[[planning-installation-types]]
== 支持的操作系统的安装类型

从{product-title}3.10开始,如果您使用RHEL作为主机的底层操作系统,那么RPM方法将用于
在该主机上安装{product-title}组件。如果使用RHEL原子主机,则在该主机上使用系统容器方法。
这两种安装类型都为集群提供了相同的功能,但是您使用的操作系统决定了
您如何管理服务和主机更新。

RPM安装通过包管理安装所有服务,并将服务配置为在相同的用户空间中运行,而系统容器安装
使用系统容器映像安装服务,并在单个容器中运行单独的服务。

当在RHEL上使用RPM时,所有服务都由包管理从外部源安装和更新。这些包修改同一用户空间中主机的现有配置。
使用RHEL原子主机上的系统容器安装,{product-title}的每个组件都作为容器(在一个自包含的包中)提供,
使用宿主的内核运行。更新的,较新的容器将替换主机上的任何现有容器。

下表和小节概述了安装类型之间的进一步差异:

.安装类型之间的差异
[cols="h,2*",options="header"]
|===
| |Red Hat Enterprise Linux | RHEL原子主机

|安装类型 |基于RPM |系统容器
|交付机制 |使用 `yum` 的RPM包 |使用 `docker` 的系统容器映像
|服务管理 |*systemd* |`docker` 和 *systemd* 单元
|===

[[containerized-required-images]]
=== 系统容器所需的映像

系统容器安装类型使用以下图像:

ifdef::openshift-origin[]
- *openshift/origin-node*
endif::[]
ifdef::openshift-enterprise[]
- *openshift3/ose-node*

默认情况下,上述所有图像都是从Red Hat注册表的
https://registry.redhat.io[registry.redhat.io].
endif::[]

如果需要在安装期间使用私有注册表来提取这些映像,
可以提前指定注册表信息。根据需要,在你的库存文件中设置以下可能的变量:

----
ifdef::openshift-origin[]
oreg_url='<registry_hostname>/openshift/origin-${component}:${version}'
endif::[]
ifdef::openshift-enterprise[]
oreg_url='<registry_hostname>/openshift3/ose-${component}:${version}'
endif::[]
openshift_docker_insecure_registries=<registry_hostname>
openshift_docker_blocked_registries=<registry_hostname>
----

[NOTE]
====
您还可以将 `openshift_docker_insecure_registries` 变量设置为主机的IP地址。
`0.0.0.0/0` 不是有效的设置。
====

默认组件从 `oreg_url` 值继承图像前缀和版本。

额外的,不安全的和被阻塞的容器注册中心的配置发生在安装过程的开始,
以确保在尝试提取任何所需的映像之前应用了这些设置。

[[planning-installation-types-service-names]]
=== systemd服务名称

安装过程创建相关的 *systemd* 单元,可以使用普通 *systemctl* 命令启动,停止和轮询服务。
对于系统容器安装,这些单元名与RPM安装的单元名匹配。

[[containerized-file-paths]]
=== 文件路径位置

所有{product-title}配置文件在容器化安装过程中都被放置在与置于RPM
的安装相同的位置,并且将在 *os-tree* 升级中存活下来。

然而,
xref:../install_config/imagestreams_templates.adoc#install-config-imagestreams-templates[默认图像流和模板文件]
是否安装在 *_/etc/origin/examples/_* 原子主机上,而不是标准的
*_/usr/share/openshift/examples/_* 上,因为该目录在RHEL上是只读的原子主机。

[[containerized-storage-requirements]]
=== 存储需求

RHEL Atomic原子主机安装通常有一个非常小的根文件系统。
但是,etcd,主容器和节点容器将数据保存在 *_/var/lib/_* 目录中。
在安装{product-title}之前,确保根文件系统上有足够的空间,请参考
xref:prerequisites.adoc#system-requirements[系统需求]小节了解详细信息。
